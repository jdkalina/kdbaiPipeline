# Progress Log - Q for Mortals RAG Agent
# Started: 2026-01-16
# Project: RAG system for Q for Mortals documentation using KDB.AI

## Codebase Patterns
<!-- Consolidate reusable patterns here as they're discovered -->

### KDB.AI
- Endpoint: http://192.168.1.68:8082
- Use `session.database("default")` to get the default database
- Schema uses `float32s` type for embedding vectors
- Use cosine similarity (`metric: "CS"`) for text embeddings
- BAAI/bge-small-en-v1.5 produces 384-dimensional vectors

### Project Conventions
- All source code in `src/` directory
- Configuration in `config/config.yaml`
- Raw scraped content in `data/raw/`
- Use SQLite for status tracking (lightweight, Python-native)
- Logging via Python `logging` module, not print statements

---

## Iteration History


### Iteration 1 - 2026-01-16T16:21:18-06:00
Progress: 0/31

### Iteration 2 - 2026-01-16T16:21:26-06:00
Progress: 0/31

### Iteration 3 - 2026-01-16T16:21:32-06:00
Progress: 0/31

### Iteration 4 - 2026-01-16T16:21:40-06:00
Progress: 0/31

### Iteration 5 - 2026-01-16T16:21:46-06:00
Progress: 0/31

### Iteration 6 - 2026-01-16T16:21:52-06:00
Progress: 0/31

### Iteration 7 - 2026-01-16T16:21:58-06:00
Progress: 0/31

### Iteration 1 - 2026-01-16T16:22:45-06:00
Progress: 0/31

### Iteration 2 - 2026-01-16T16:22:52-06:00
Progress: 0/31

### Iteration 3 - 2026-01-16T16:22:58-06:00
Progress: 0/31

### Iteration 4 - 2026-01-16T16:23:04-06:00
Progress: 0/31

### Iteration 5 - 2026-01-16T16:23:10-06:00
Progress: 0/31

### Iteration 6 - 2026-01-16T16:23:17-06:00
Progress: 0/31

### Iteration 7 - 2026-01-16T16:23:23-06:00
Progress: 0/31

### Iteration 8 - 2026-01-16T16:23:29-06:00
Progress: 0/31

### Iteration 9 - 2026-01-16T16:23:35-06:00
Progress: 0/31

### Iteration 10 - 2026-01-16T16:23:42-06:00
Progress: 0/31

---

### Story: setup-01 - Create project directory structure
**Completed:** 2026-01-16T16:35:00-06:00

**What was implemented:**
- Verified existing project directory structure
- All required directories already existed from prior setup:
  - `src/` with subdirectories: `api/`, `chat/`, `embedding/`, `kdbai/`, `scraper/`
  - `data/raw/` and `data/processed/`
  - `config/`
  - `tests/`
- Each src subdirectory has an `__init__.py` file

**Files present:**
- src/__init__.py
- src/api/__init__.py
- src/chat/__init__.py
- src/embedding/__init__.py
- src/kdbai/__init__.py
- src/scraper/__init__.py
- tests/__init__.py

**Learnings for future iterations:**
- Directory structure was created in an earlier iteration
- The structure follows the pattern specified in prompt.md

### Iteration 1 - 2026-01-16T16:33:15-06:00
Progress: 1/31

### Story: setup-02 - Create config.yaml with project settings
**Completed:** 2026-01-16T16:40:00-06:00

**What was implemented:**
- Created config/config.yaml with comprehensive project settings
- Scraper settings: base_url, delay, timeout, CSS selectors for content extraction
- Embedding settings: model_name (BAAI/bge-small-en-v1.5), dimensions (384), chunk_size (512), overlap
- KDB.AI settings: endpoint, database, table_name, index configuration, batch sizes
- API settings: host, port, default/max top_k
- Chat settings: Gradio server config, LLM provider options (ollama/openai)
- Database settings: SQLite path
- Logging settings: level, format

**Files modified:**
- config/config.yaml (created)

**Learnings for future iterations:**
- YAML validated successfully with PyYAML
- Config structure supports both ollama and openai LLM providers
- All settings have sensible defaults that can be overridden by environment variables where appropriate


### Iteration 2 - 2026-01-16T16:34:46-06:00
Progress: 2/31

### Story: setup-03 - Create SQLite database schema
**Completed:** 2026-01-16T16:45:00-06:00

**What was implemented:**
- Created src/db.py with SQLite schema for status tracking
- Schema includes: file_id (PK), url (unique), filename, scraped_at, content_hash, status, error_message
- Added init_db() function that creates the database and tables
- Added indexes on status and url columns for efficient querying
- Used sqlite3.Row factory for dict-like access to rows
- Used Python logging module for status messages

**Files modified:**
- src/db.py (created)

**Learnings for future iterations:**
- Database path configured in config/config.yaml as "data/status.db"
- init_db() creates parent directory if it doesn't exist
- Row factory enables accessing columns by name (e.g., row['status'])
- Indexes added proactively for status filtering (needed by embedding pipeline)

### Iteration 3 - 2026-01-16T16:45:00-06:00
Progress: 3/31

### Iteration 3 - 2026-01-16T16:36:19-06:00
Progress: 3/31

### Story: setup-04 - Create requirements.txt with dependencies
**Completed:** 2026-01-16T16:50:00-06:00

**What was implemented:**
- Created requirements.txt with all required project dependencies
- Web scraping: requests, beautifulsoup4, markdownify
- Embeddings: fastembed
- KDB.AI: kdbai-client
- API server: fastapi, uvicorn
- Configuration: pyyaml
- Chat interface: gradio
- LLM providers: openai, httpx (for ollama async HTTP)
- Development/testing: pytest, mypy

**Files modified:**
- requirements.txt (created)

**Learnings for future iterations:**
- pip --dry-run validates the requirements file successfully
- sqlite3 is part of Python standard library, no need to include
- httpx added for async HTTP support needed by ollama client
- Version constraints use >= to allow flexibility while ensuring minimum compatible versions

### Iteration 4 - 2026-01-16T16:50:00-06:00
Progress: 4/31

### Iteration 4 - 2026-01-16T16:37:45-06:00
Progress: 4/31

### Story: scrape-01 - Implement URL discovery for Q4M chapters
**Completed:** 2026-01-16T16:39:48-06:00

**What was implemented:**
- Created src/scraper/discover.py with discover_chapter_urls() function
- Fetches the Q4M3 index page and extracts all chapter URLs
- Uses a known list of chapter patterns for reliable discovery
- Filters out anchor links (section links within pages)
- Returns list of 19 absolute URLs for main chapter pages
- Includes fallback to known chapter list if dynamic discovery fails
- Uses Python logging module for status messages

**Files modified:**
- src/scraper/discover.py (created)

**Learnings for future iterations:**
- Q4M3 has 19 chapters: preface, 0-14 numbered chapters, A & B appendices, and colophon
- Chapter URLs follow pattern: base_url + "X_Chapter_Name/"
- The site uses relative links that need urljoin() to make absolute
- Chapter 14 uses URL encoding for "+" (%2B in some links)
- Skip anchor links (containing #) when discovering main chapter pages

### Iteration 5 - 2026-01-16T16:39:48-06:00
Progress: 5/31

### Iteration 5 - 2026-01-16T16:40:31-06:00
Progress: 5/31

### Story: scrape-02 - Implement HTML fetcher with rate limiting
**Completed:** 2026-01-16T16:41:37-06:00

**What was implemented:**
- Created src/scraper/fetcher.py with fetch_page() function and Fetcher class
- Rate limiting: configurable delay between requests (default 1 second)
- Fetcher class maintains rate limit state across multiple requests
- fetch_page() convenience function for one-off fetches
- Proper error handling for Timeout, HTTPError, and RequestException
- Custom User-Agent header identifying the scraper
- Context manager support (with statement) for Fetcher class
- Uses Python logging module for status messages

**Files modified:**
- src/scraper/fetcher.py (created)

**Learnings for future iterations:**
- Fetcher class is preferred for multiple requests (maintains session and rate limit state)
- fetch_page() is a simpler interface for single requests
- Config settings: scraper.delay (1.0s), scraper.timeout (30s)
- Session-based fetcher allows connection reuse for better performance

### Iteration 6 - 2026-01-16T16:41:37-06:00
Progress: 6/31

### Iteration 6 - 2026-01-16T16:42:25-06:00
Progress: 6/31

### Story: scrape-03 - Implement content extraction from HTML
**Completed:** 2026-01-16T17:00:00-06:00

**What was implemented:**
- Created src/scraper/extractor.py with extract_body(html) function
- Uses BeautifulSoup to parse HTML and extract main content area
- Configurable content selectors (main, article, div.md-content, div.content)
- Configurable remove selectors for non-content elements (nav, header, footer, aside, sidebar, toc, scripts, styles)
- Falls back to <body> if no main content area found
- Returns clean HTML string with only the main content
- Comprehensive logging of extraction progress

**Files modified:**
- src/scraper/extractor.py (created)

**Learnings for future iterations:**
- BeautifulSoup's select_one() returns None if no match, allows simple fallback chain
- decompose() removes elements permanently from the tree
- Config has content and remove selectors in scraper.selectors section
- The function returns HTML (not markdown) - conversion is handled separately in scrape-04

### Iteration 7 - 2026-01-16T17:00:00-06:00
Progress: 7/31

### Iteration 7 - 2026-01-16T16:43:45-06:00
Progress: 7/31

### Story: scrape-04 - Implement HTML to Markdown conversion
**Completed:** 2026-01-16T17:05:00-06:00

**What was implemented:**
- Added convert_to_markdown(html) function to src/scraper/extractor.py
- Uses markdownify library for HTML to Markdown conversion
- Configurable options: code_language (default "q"), heading_style ("ATX" for # symbols)
- Cleans up excessive blank lines (max 2 consecutive)
- Strips leading/trailing whitespace
- Added extract_and_convert() convenience function combining extraction and conversion
- Updated test section to demonstrate all three functions

**Files modified:**
- src/scraper/extractor.py (updated)

**Learnings for future iterations:**
- markdownify handles code blocks, headings, lists, bold, italic out of the box
- The code_language parameter sets default language for code blocks
- ATX heading style uses # symbols (H1=1#, H2=2#, etc)
- markdownify's strip parameter removes specified tags entirely
- Excessive blank lines need post-processing cleanup

### Iteration 8 - 2026-01-16T17:05:00-06:00
Progress: 8/31

### Iteration 8 - 2026-01-16T16:45:31-06:00
Progress: 8/31

### Story: scrape-05 - Implement file saving with unique filenames
**Completed:** 2026-01-16T17:10:00-06:00

**What was implemented:**
- Created src/scraper/storage.py with file saving functionality
- url_to_filename(url): Generates slug-based filenames from URLs
  - Decodes URL-encoded characters (e.g., %2B -> +)
  - Replaces + with _plus to preserve meaning (e.g., kdb+ -> kdb_plus)
  - Sanitizes characters to filesystem-safe equivalents
- save_content(url, content, output_dir): Saves content to data/raw/ by default
- file_exists(url, output_dir): Checks if file already exists for a URL
- get_file_path(url, output_dir): Returns full Path object for a URL
- read_content(url, output_dir): Reads content back from file

**Files modified:**
- src/scraper/storage.py (created)

**Learnings for future iterations:**
- Use urllib.parse.unquote() to decode URL-encoded characters before slugifying
- The + symbol is special in kdb+ documentation - preserve it as "_plus"
- pathlib.Path used consistently for all file operations
- File storage is decoupled from database tracking (scrape-06/07 handle that)

### Iteration 9 - 2026-01-16T17:10:00-06:00
Progress: 9/31

### Iteration 9 - 2026-01-16T16:47:55-06:00
Progress: 9/31

### Story: scrape-06 - Implement content hash for change detection
**Completed:** 2026-01-16T17:15:00-06:00

**What was implemented:**
- Added compute_hash(content) function to src/scraper/storage.py
- Uses SHA256 for cryptographic hashing (256-bit, 64-character hex output)
- Normalizes line endings (CRLF → LF) before hashing for cross-platform consistency
- Returns hexadecimal string representation of the hash
- Added comprehensive tests verifying:
  - Same content produces identical hashes
  - Different content produces different hashes
  - Line ending normalization works correctly

**Files modified:**
- src/scraper/storage.py (updated)

**Learnings for future iterations:**
- SHA256 produces 64-character hex strings, suitable for database storage
- Line ending normalization is important for content comparison across OSes
- hashlib is part of Python standard library, no additional dependencies needed
- The hash will be stored in SQLite content_hash column for idempotency checks

### Iteration 10 - 2026-01-16T17:15:00-06:00
Progress: 10/31

### Iteration 10 - 2026-01-16T16:49:21-06:00
Progress: 10/31

### Story: scrape-07 - Implement status database operations
**Completed:** 2026-01-16T17:20:00-06:00

**What was implemented:**
- Added `insert_scraped_file()` function to insert new file records with auto-generated UUID file_id and ISO timestamp
- Added `get_file_by_url()` function to retrieve file records by URL (returns None if not found)
- Added `update_status()` function to update file status with optional error_message
- Added `get_files_by_status()` function to retrieve all files matching a status
- Added `update_content_hash()` function as bonus utility for idempotency (updates hash and scraped_at timestamp)
- All functions use parameterized queries to prevent SQL injection
- Added uuid and datetime imports

**Files modified:**
- src/db.py (updated)

**Learnings for future iterations:**
- sqlite3.Row allows dict-like access (row['column_name'])
- cursor.rowcount tells us if UPDATE affected any rows
- Use UUID4 for file_id to ensure uniqueness across runs
- datetime.utcnow().isoformat() produces standard ISO format timestamps
- Status flow: pending → scraped → embedded (or error at any point)

### Iteration 11 - 2026-01-16T17:20:00-06:00
Progress: 11/31

### Iteration 11 - 2026-01-16T16:51:03-06:00
Progress: 11/31

### Iteration 12 - 2026-01-16T16:51:50-06:00
Progress: 11/31

### Iteration 13 - 2026-01-16T16:51:57-06:00
Progress: 11/31

### Iteration 14 - 2026-01-16T16:52:05-06:00
Progress: 11/31

### Iteration 15 - 2026-01-16T16:52:12-06:00
Progress: 11/31

### Iteration 16 - 2026-01-16T16:52:19-06:00
Progress: 11/31

### Iteration 17 - 2026-01-16T16:52:26-06:00
Progress: 11/31

### Iteration 18 - 2026-01-16T16:52:33-06:00
Progress: 11/31

### Iteration 19 - 2026-01-16T16:52:40-06:00
Progress: 11/31

### Iteration 20 - 2026-01-16T16:52:46-06:00
Progress: 11/31

### Iteration 21 - 2026-01-16T16:52:53-06:00
Progress: 11/31

### Iteration 22 - 2026-01-16T16:53:00-06:00
Progress: 11/31

### Iteration 23 - 2026-01-16T16:53:07-06:00
Progress: 11/31

### Iteration 24 - 2026-01-16T16:53:14-06:00
Progress: 11/31

### Iteration 25 - 2026-01-16T16:53:21-06:00
Progress: 11/31

### Iteration 26 - 2026-01-16T16:53:28-06:00
Progress: 11/31

### Iteration 27 - 2026-01-16T16:53:35-06:00
Progress: 11/31

### Iteration 28 - 2026-01-16T16:53:42-06:00
Progress: 11/31

### Iteration 29 - 2026-01-16T16:53:49-06:00
Progress: 11/31

### Iteration 30 - 2026-01-16T16:53:56-06:00
Progress: 11/31

### Story: scrape-08 - Create main scraper orchestrator
**Completed:** 2026-01-16T17:00:00-06:00

**What was implemented:**
- Created src/scraper/main.py with full scrape pipeline orchestration
- Loads configuration from config/config.yaml
- Discovers chapter URLs using discover_chapter_urls()
- Fetches each page with rate limiting using Fetcher class
- Extracts and converts HTML to Markdown using extract_and_convert()
- Computes content hash for each file using compute_hash()
- Saves content to data/raw/ using save_content()
- Updates SQLite database with insert_scraped_file()
- Includes comprehensive progress logging with statistics
- CLI entry point with argparse for --config, --output-dir, --db-path flags
- Returns statistics dict with total, scraped, skipped, failed counts
- Already-scraped URLs are skipped (checks database first)

**Files modified:**
- src/scraper/main.py (created)

**Learnings for future iterations:**
- The scraper checks if URL already exists in database before scraping
- Uses context manager (with Fetcher) to ensure session cleanup
- All components are loosely coupled via function calls
- Statistics tracking enables easy monitoring of scrape progress
- Exit code 1 if any failures occurred, useful for CI/CD

### Iteration 31 - 2026-01-16T17:00:00-06:00
Progress: 12/31

### Story: scrape-09 - Add idempotency check to scraper
**Completed:** 2026-01-16T21:11:23-06:00

**What was implemented:**
- Updated src/scraper/main.py to check content hash before re-scraping
- Added import for `update_content_hash` from src.db
- Modified scraping loop to:
  1. Always fetch and extract content (needed for hash comparison)
  2. Compare new hash against stored hash in database
  3. Skip if hash matches (content unchanged)
  4. Update file and database if hash differs (content changed)
  5. Insert new record if URL not in database
- Added new "updated" statistic to track changed files
- Updated summary logging with clearer labels: New, Updated, Unchanged, Failed
- Updated docstring to document idempotency behavior

**Files modified:**
- src/scraper/main.py

**Learnings for future iterations:**
- Idempotency check requires fetching content first (can't skip HTTP request entirely)
- The `update_content_hash()` function already existed in db.py from scrape-07
- Rate limiting is preserved since we still go through the Fetcher class
- Content hash comparison uses first 12 chars in logs for readability
- Stats renamed: "scraped" = new files, "skipped" = unchanged (better semantics)

### Iteration 2 - 2026-01-16T21:11:23-06:00
Progress: 13/31

### Iteration 2 - 2026-01-16T21:12:03-06:00
Progress: 13/31

### Story: embed-01 - Implement heading-aware text chunker
**Completed:** 2026-01-16T21:15:00-06:00

**What was implemented:**
- Created src/embedding/chunker.py with heading-aware text chunking
- Chunk dataclass with metadata: text, heading_h1, heading_h2, heading_h3, chunk_index, source_file
- `chunk_by_headings(markdown, source_file)` function that splits on H1/H2/H3 headings
- Preserves heading hierarchy metadata for each chunk (e.g., "Chapter 1 > Section 1.1 > Subsection 1.1.1")
- Includes heading text in each chunk for context
- Helper properties: `heading_hierarchy` (full path), `heading` (most specific heading)
- Comprehensive logging of chunk creation

**Files modified:**
- src/embedding/chunker.py (created)

**Learnings for future iterations:**
- Regex `^(#{1,3})\s+(.+)$` reliably matches ATX-style headings
- Heading hierarchy resets lower levels when a higher level is encountered (H1 resets H2/H3)
- Include the heading line itself in the chunk for better embedding context
- The Chunk dataclass provides clean API for accessing metadata
- embed-02 will need to add size limits with overlap to this foundation

### Iteration 3 - 2026-01-16T21:15:00-06:00
Progress: 14/31

### Iteration 3 - 2026-01-16T21:13:56-06:00
Progress: 14/31

### Story: embed-02 - Add chunk size limits with overlap
**Completed:** 2026-01-16T21:16:22-06:00

**What was implemented:**
- Added `estimate_tokens(text)` function using ~4 chars/token heuristic for English text
- Added `split_text_with_overlap(text, max_tokens, overlap_tokens)` function that:
  - Splits by paragraphs first, then sentences, then lines, then words as fallback
  - Maintains configurable overlap between chunks for context continuity
  - Returns original text unchanged if under token limit
- Updated `chunk_by_headings()` to accept optional `max_tokens` and `overlap_tokens` parameters
- Loads default values from config/config.yaml (chunk_size: 512, chunk_overlap: 50)
- Two-phase chunking: first by headings, then split oversized sections with overlap
- Each resulting chunk maintains full heading hierarchy metadata

**Files modified:**
- src/embedding/chunker.py

**Learnings for future iterations:**
- Token estimation using 4 chars/token is a reasonable heuristic for embedding models
- Splitting preserves structure: paragraphs > sentences > lines > words
- Overlap is computed from end of current chunk to start of next chunk
- The config values (chunk_size=512, chunk_overlap=50) align with typical embedding model contexts
- Test confirmed overlap works: consecutive chunks share words at boundaries

### Iteration 4 - 2026-01-16T21:16:22-06:00
Progress: 15/31

### Iteration 4 - 2026-01-16T21:17:01-06:00
Progress: 15/31

### Story: embed-03 - Implement FastEmbed wrapper
**Completed:** 2026-01-16T21:20:00-06:00

**What was implemented:**
- Created src/embedding/embedder.py with FastEmbed wrapper
- Embedder class with lazy model loading for memory efficiency
- `generate_embeddings(texts)` method for batch embedding generation
- `generate_embedding(text)` method for single text embedding
- Singleton pattern via `get_embedder()` for simple usage
- Convenience functions `generate_embeddings()` and `generate_embedding()`
- Configuration loaded from config/config.yaml (model_name, batch_size, dimensions)
- Default model: BAAI/bge-small-en-v1.5 (384 dimensions)
- Added numpy to requirements.txt (required by fastembed)

**Files modified:**
- src/embedding/embedder.py (created)
- requirements.txt (added numpy dependency)

**Learnings for future iterations:**
- FastEmbed's embed() returns a generator, must convert to list
- Lazy model loading avoids memory allocation until first use
- The model outputs numpy arrays of shape (384,) for bge-small-en-v1.5
- Singleton embedder avoids reloading model for multiple calls


### Iteration 5 - 2026-01-16T21:18:54-06:00
Progress: 16/31

### Story: embed-04 - Create KDB.AI client wrapper
**Completed:** 2026-01-16T21:25:00-06:00

**What was implemented:**
- Created src/kdbai/client.py with KDB.AI client wrapper
- KDBAIClient class with methods:
  - `connect()`: Connect to KDB.AI server
  - `create_table()`: Create table with schema and indexes
  - `get_table()`: Get reference to existing table
  - `insert_batch()`: Insert records into table
  - `search()`: Vector similarity search with optional filters
  - `close()`: Close connection
- Configuration loaded from config/config.yaml (endpoint, database, table_name, index settings)
- Endpoint can be overridden via KDBAI_ENDPOINT environment variable
- Module-level convenience functions: `connect()`, `create_table()`, `insert_batch()`, `search()`
- Singleton pattern via `get_client()` for simple usage
- Comprehensive logging throughout

**Files modified:**
- src/kdbai/client.py (created)

**Learnings for future iterations:**
- KDB.AI uses `float32s` type for embedding vectors
- Endpoint priority: function arg > env var > config file
- Session and database handles are cached for connection reuse
- Table existence check: `[t.name for t in db.tables]`
- Search results come as pandas DataFrame, convert to list of dicts
- embed-05 will add the specific table schema creation function

### Iteration 6 - 2026-01-16T21:25:00-06:00
Progress: 17/31

### Iteration 6 - 2026-01-16T21:20:35-06:00
Progress: 17/31

### Story: embed-05 - Define KDB.AI table schema
**Completed:** 2026-01-16T21:21:24-06:00

**What was implemented:**
- Added `create_embeddings_table()` convenience function to src/kdbai/client.py
- Documents the standard schema for Q4M chunks:
  - chunk_id (str): Unique identifier for each chunk
  - text (str): The actual text content of the chunk
  - chapter (str): Chapter name/title from the source document
  - heading (str): Section heading hierarchy
  - url (str): Source URL where the content was scraped from
  - file_id (str): Reference to the file in the SQLite status database
  - embeddings (float32s): Vector embeddings for similarity search
- The function is a thin wrapper around create_table() with good documentation
- Table includes flat index on embeddings using cosine similarity (CS)

**Files modified:**
- src/kdbai/client.py

**Learnings for future iterations:**
- The schema was already defined in create_table() from embed-04
- This story adds a well-documented convenience function for discoverability
- Schema uses float32s type for vectors (KDB.AI specific)
- Index configuration comes from config: flat index, 384 dims, cosine similarity

### Iteration 7 - 2026-01-16T21:21:24-06:00
Progress: 18/31

### Iteration 7 - 2026-01-16T21:22:00-06:00
Progress: 18/31

### Story: embed-06 - Implement batch insert to KDB.AI
**Completed:** 2026-01-16T21:30:00-06:00

**What was implemented:**
- Added `insert_chunks(chunks_with_embeddings)` method to KDBAIClient class
- Method batches inserts in groups of 100 (configurable via `insert_batch_size` in config)
- Uses ceiling division to calculate number of batches needed
- Progress logging shows batch number, records inserted per batch, and running total
- Added corresponding module-level convenience function `insert_chunks()`
- Comprehensive docstrings with schema documentation and usage examples

**Files modified:**
- src/kdbai/client.py

**Learnings for future iterations:**
- The `insert_batch_size` config setting (default 100) controls batch size
- Ceiling division: `(total + batch_size - 1) // batch_size` handles remainders
- Progress logging at INFO level for start/end, DEBUG for per-batch details
- The method returns total count of inserted records for verification

### Iteration 8 - 2026-01-16T21:30:00-06:00
Progress: 19/31

### Iteration 8 - 2026-01-16T21:23:27-06:00
Progress: 19/31

### Story: embed-07 - Create embedding pipeline orchestrator
**Completed:** 2026-01-16T21:35:00-06:00

**What was implemented:**
- Created src/embedding/main.py with full embedding pipeline orchestration
- `run_pipeline()` function orchestrates the full embedding pipeline:
  1. Polls SQLite database for files with status='scraped'
  2. Reads scraped markdown content from data/raw/
  3. Chunks content using heading-aware chunking from chunker.py
  4. Generates embeddings using FastEmbed wrapper from embedder.py
  5. Inserts chunks with embeddings into KDB.AI using client.py
  6. Updates file status to 'embedded' in SQLite
- `process_file()` helper handles individual file processing
- `prepare_chunk_record()` creates records matching KDB.AI schema
- CLI entry point with argparse: --config, --drop-table, --no-create-table, --verbose
- Returns statistics dict with files_processed, files_failed, total_chunks, total_inserted
- Proper error handling with status update to 'error' on failure

**Files modified:**
- src/embedding/main.py (created)

**Learnings for future iterations:**
- The pipeline uses lazy loading for the embedding model (first use triggers load)
- KDB.AI table is created automatically if it doesn't exist
- read_content() from storage.py can read files by URL
- Each chunk gets a UUID for chunk_id
- numpy arrays need .tolist() conversion before JSON serialization
- embed-08 will add more detailed progress logging

### Iteration 9 - 2026-01-16T21:35:00-06:00
Progress: 20/31

### Story: embed-08 - Add progress logging to embedding pipeline
**Completed:** 2026-01-16T21:40:00-06:00

**What was implemented:**
- Enhanced structured logging throughout the embedding pipeline
- Added pipeline start banner with configuration summary (model, chunk size, overlap, table name)
- Added file progress indicators: `[X/Y]` format for each file processed
- Added step-by-step logging in process_file():
  - Reading file (with character count)
  - Chunking complete (with chunk count)
  - Embedding complete (with embedding count)
  - Insert complete (with insert count)
- Added debug-level logs for intermediate steps (enable with --verbose)
- Updated completion/failure logs with progress indicators
- Final summary shows files_processed, files_failed, total_chunks, total_inserted

**Files modified:**
- src/embedding/main.py

**Learnings for future iterations:**
- Indented sub-step logs (2-space prefix) improve readability
- Progress indicators [X/Y] help track long-running processes
- DEBUG level is good for verbose intermediate steps
- Pipeline start banner summarizes key config for easy verification

### Iteration 10 - 2026-01-16T21:40:00-06:00
Progress: 21/31

### Story: query-01 - Create FastAPI app skeleton
**Completed:** 2026-01-16T21:45:00-06:00

**What was implemented:**
- Created src/api/main.py with FastAPI application
- GET /health endpoint returning status, version, and service name
- CORS middleware configured to allow all origins (development mode)
- Startup and shutdown event handlers with logging
- Configuration loaded from config/config.yaml
- main() function to run server with uvicorn
- API title, description, and version metadata

**Files modified:**
- src/api/main.py (created)

**Learnings for future iterations:**
- FastAPI app is created at module level for uvicorn import
- CORS middleware should be restricted in production
- @app.on_event("startup") and @app.on_event("shutdown") for lifecycle hooks
- uvicorn.run() with string "module:app" format for reload support

### Iteration 11 - 2026-01-16T21:45:00-06:00
Progress: 22/31

### Story: query-02 - Implement query embedding generation
**Completed:** 2026-01-16T21:50:00-06:00

**What was implemented:**
- Created src/api/query.py with query processing functions
- `embed_query(text)` generates embedding for user query using same FastEmbed model
- Uses lazy-loaded embedder via get_query_embedder()
- Converts numpy array to list for JSON serialization
- Validates that query text is not empty
- Also added `search_similar()` helper function for convenience (combines embed + search)
- Includes chapter_filter support for filtered searches

**Files modified:**
- src/api/query.py (created)

**Learnings for future iterations:**
- Using the same model for queries and documents is critical for semantic search
- Lazy loading avoids model load on import
- search_similar() combines embedding and search in one call
- The 384-dimension embedding matches BAAI/bge-small-en-v1.5 output

### Iteration 12 - 2026-01-16T21:50:00-06:00
Progress: 23/31

### Story: query-03 - Implement vector search endpoint
**Completed:** 2026-01-16T21:55:00-06:00

**What was implemented:**
- Added POST /query endpoint to src/api/main.py
- Created Pydantic models for request/response validation:
  - QueryRequest: query (str, required), top_k (int, 1-20, default 5)
  - SearchResult: chunk_id, text, chapter, heading, url
  - QueryResponse: query, results, count
- Endpoint calls search_similar() from query.py
- Formats KDB.AI results into structured response
- Logs each query with first 50 chars and top_k value

**Files modified:**
- src/api/main.py

**Learnings for future iterations:**
- Pydantic Field() with ge/le for validation constraints
- response_model parameter auto-validates output
- Use .get() with defaults for safe dict access from KDB.AI results
- min_length=1 prevents empty query strings

### Iteration 13 - 2026-01-16T21:55:00-06:00
Progress: 24/31
